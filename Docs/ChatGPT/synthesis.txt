There are several machine learning techniques that can be used for time-series modeling, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, autoregressive integrated moving average (ARIMA) models, and support vector regression (SVR), among others. The best technique to use depends on the specifics of the data, the size of the dataset, the level of accuracy required, and other factors.

To simulate a time-series with the same Hurst exponent, average, and standard deviation, you could use a technique known as fractional Gaussian noise (fGn). fGn is a stochastic process that has a specified Hurst exponent and a Gaussian distribution. It can be generated using a random walk, which is a simple stochastic process that can be modeled using an ARIMA model or other time-series models.

Alternatively, you could use a generative adversarial network (GAN) to generate synthetic time-series data that has similar statistical properties to the original data. GANs consist of two neural networks: a generator and a discriminator. The generator generates synthetic data that is similar to the original data, while the discriminator tries to distinguish between the original and synthetic data. The two networks are trained iteratively until the generator produces synthetic data that is statistically indistinguishable from the original data.

As for analytical techniques, statistical tests such as the Kolmogorov-Smirnov test, the Anderson-Darling test, and the Shapiro-Wilk test can be used to test the normality of the data and the similarity between the generated and original data. Other techniques such as the autocorrelation function and power spectral density analysis can also be used to analyze the statistical properties of the time-series data.